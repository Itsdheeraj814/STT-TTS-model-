{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0fd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import webrtcvad\n",
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from TTS.api import TTS\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e25c700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Defaults & constants\n",
    "# ----------------------\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "DEFAULT_MODEL = \"openai/whisper-medium\"\n",
    "DEFAULT_SAMPLE_RATE = 16000\n",
    "DEFAULT_FRAME_MS = 20\n",
    "DEFAULT_VAD_AGGR = 2\n",
    "DEFAULT_MAX_SILENCE_FRAMES = 12  # ~240ms\n",
    "TRANSCRIPT_CSV = \"transcripts.csv\"\n",
    "LOG_FILE = \"realtime_translator.log\"\n",
    "\n",
    "\n",
    "\n",
    "# Filler/short outputs to ignore\n",
    "IGNORE_SET = {\"thank you\", \"thanks\", \"ok\", \"okay\", \"hmm\", \"mm\", \"mhm\", \"yeah\", \"no\", \"nah\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7244fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(logfile: str = LOG_FILE, level=logging.INFO):\n",
    "    logger = logging.getLogger(\"realtime_translator\")\n",
    "    logger.setLevel(level)\n",
    "    fmt = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(fmt)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    fh = RotatingFileHandler(logfile, maxBytes=5 * 1024 * 1024, backupCount=3)\n",
    "    fh.setFormatter(fmt)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logger()\n",
    "\n",
    "def parse_args():\n",
    "    class Args:\n",
    "        model = DEFAULT_MODEL\n",
    "        sample_rate = DEFAULT_SAMPLE_RATE\n",
    "        frame_ms = DEFAULT_FRAME_MS\n",
    "        vad_aggr = DEFAULT_VAD_AGGR\n",
    "        lang = None\n",
    "        task = \"translate\"\n",
    "        max_silence_frames = DEFAULT_MAX_SILENCE_FRAMES\n",
    "        num_beams = 5\n",
    "        output_csv = TRANSCRIPT_CSV\n",
    "        no_tts = False\n",
    "    return Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf41ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transcript(csv_path, timestamp, input_lang, task, text):\n",
    "    write_header = not os.path.exists(csv_path)\n",
    "    with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if write_header:\n",
    "            w.writerow([\"timestamp\", \"input_lang\", \"task\", \"text\"])\n",
    "        w.writerow([timestamp, input_lang or \"\", task, text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb0a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealtimeTranslator:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.sample_rate = args.sample_rate\n",
    "        self.frame_ms = args.frame_ms\n",
    "        self.chunk_samples = int(self.sample_rate * self.frame_ms / 1000)\n",
    "        self.vad = webrtcvad.Vad(args.vad_aggr)\n",
    "        self.q = queue.Queue()\n",
    "        self.running = threading.Event()\n",
    "        self.running.set()\n",
    "        self.silence_threshold = args.max_silence_frames\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "        logger.info(f\"Using device={self.device} dtype={self.dtype}\")\n",
    "\n",
    "        logger.info(\"Loading Whisper model and processor...\")\n",
    "        self.processor = WhisperProcessor.from_pretrained(args.model, use_auth_token=HF_TOKEN)\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(\n",
    "            args.model, torch_dtype=self.dtype, use_auth_token=HF_TOKEN\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        gen_kw = dict(\n",
    "            task=args.task,\n",
    "            language=args.lang,\n",
    "            num_beams=args.num_beams,\n",
    "            temperature=0.0,\n",
    "            no_repeat_ngram_size=3,\n",
    "            min_length=4,\n",
    "            length_penalty=1.0,\n",
    "            suppress_tokens=[],\n",
    "        )\n",
    "        self.gen_kw = gen_kw\n",
    "\n",
    "        self.no_tts = args.no_tts\n",
    "        if not self.no_tts:\n",
    "            logger.info(\"Loading TTS model...\")\n",
    "            try:\n",
    "                self.tts = TTS(\n",
    "                    model_name=\"tts_models/multilingual/multi-dataset/your_tts\",\n",
    "                    progress_bar=False, gpu=(self.device == \"cuda\")\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.exception(\"Failed to load TTS model - continuing in no-tts mode.\")\n",
    "                self.no_tts = True\n",
    "                self.tts = None\n",
    "        else:\n",
    "            self.tts = None\n",
    "\n",
    "        self.default_speaker = self.tts.speakers[0] if (self.tts and len(self.tts.speakers) > 0) else None\n",
    "        logger.info(f\"Default speaker: {self.default_speaker}\")\n",
    "\n",
    "        self.executor = ThreadPoolExecutor(max_workers=2)\n",
    "        self._warmup()\n",
    "\n",
    "    def _warmup(self):\n",
    "        try:\n",
    "            dummy = np.zeros((1600,), dtype=np.float32)\n",
    "            inputs = self.processor(dummy, sampling_rate=self.sample_rate, return_tensors=\"pt\")\n",
    "            input_features = inputs.input_features.to(self.device, dtype=self.dtype)\n",
    "            with torch.no_grad():\n",
    "                _ = self.model.generate(input_features, max_length=1, **self.gen_kw)\n",
    "            logger.info(\"Warmup complete.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Warmup failed: %s\", e)\n",
    "\n",
    "    def audio_callback(self, indata, frames, time_info, status):\n",
    "        if status:\n",
    "            logger.debug(\"Input status: %s\", status)\n",
    "        self.q.put(bytes(indata))\n",
    "\n",
    "    def bytes_to_tensor(self, fr):\n",
    "        arr = np.frombuffer(fr, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "        return torch.from_numpy(arr).to(self.device, dtype=self.dtype)\n",
    "\n",
    "    def preemphasis_torch(self, x: torch.Tensor, coeff: float = 0.97):\n",
    "        if x.numel() == 0:\n",
    "            return x\n",
    "        return torch.cat([x[:1], x[1:] - coeff * x[:-1]])\n",
    "\n",
    "    def tts_playback_worker(self, text, out_path=\"output.wav\"):\n",
    "        try:\n",
    "            tts_lang = \"en\" if self.args.task == \"translate\" else None\n",
    "            self.tts.tts_to_file(text=text, file_path=out_path,\n",
    "                                 speaker=self.default_speaker, language=tts_lang)\n",
    "            data, sr = sf.read(out_path)\n",
    "            sd.play(data, sr)\n",
    "            sd.wait()\n",
    "            os.remove(out_path)\n",
    "        except Exception:\n",
    "            logger.exception(\"TTS/playback failed for text: %s\", text)\n",
    "\n",
    "    def run(self):\n",
    "        logger.info(\"Starting real-time loop. Press Stop to interrupt.\")\n",
    "        ring = []\n",
    "        silence_count = 0\n",
    "        try:\n",
    "            with sd.RawInputStream(\n",
    "                samplerate=self.sample_rate,\n",
    "                blocksize=self.chunk_samples,\n",
    "                dtype=\"int16\",\n",
    "                channels=1,\n",
    "                callback=self.audio_callback,\n",
    "            ):\n",
    "                while self.running.is_set():\n",
    "                    try:\n",
    "                        frame = self.q.get(timeout=0.2)\n",
    "                    except queue.Empty:\n",
    "                        continue\n",
    "\n",
    "                    is_speech = self.vad.is_speech(frame, self.sample_rate)\n",
    "                    tensor_frame = self.bytes_to_tensor(frame)\n",
    "\n",
    "                    if is_speech:\n",
    "                        ring.append(tensor_frame)\n",
    "                        silence_count = 0\n",
    "                    else:\n",
    "                        if len(ring) > 0:\n",
    "                            silence_count += 1\n",
    "                            if silence_count < self.silence_threshold:\n",
    "                                ring.append(tensor_frame)\n",
    "                                continue\n",
    "\n",
    "                            speech = torch.cat(ring)\n",
    "                            ring.clear()\n",
    "                            silence_count = 0\n",
    "\n",
    "                            speech = self.preemphasis_torch(speech)\n",
    "                            inputs = self.processor(\n",
    "                                speech.cpu().numpy(),\n",
    "                                sampling_rate=self.sample_rate,\n",
    "                                return_tensors=\"pt\"\n",
    "                            )\n",
    "                            input_features = inputs.input_features.to(self.device, dtype=self.dtype)\n",
    "\n",
    "                            with torch.no_grad():\n",
    "                                generated_ids = self.model.generate(input_features, **self.gen_kw)\n",
    "\n",
    "                            text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "                            if not text:\n",
    "                                continue\n",
    "\n",
    "                            tlow = text.lower().strip(\" .!,?\")\n",
    "                            if tlow in IGNORE_SET or (len(tlow.split()) <= 1 and len(tlow) < 3):\n",
    "                                continue\n",
    "\n",
    "                            ts = datetime.utcnow().isoformat()\n",
    "                            logger.info(\"Result [%s]: %s\", ts, text)\n",
    "\n",
    "                            save_transcript(self.args.output_csv, ts, self.args.lang, self.args.task, text)\n",
    "\n",
    "                            if not self.no_tts and self.tts:\n",
    "                                outpath = f\"output_{int(time.time()*1000)}.wav\"\n",
    "                                self.executor.submit(self.tts_playback_worker, text, outpath)\n",
    "\n",
    "        except Exception:\n",
    "            logger.exception(\"Unhandled exception in main loop.\")\n",
    "        finally:\n",
    "            self.shutdown()\n",
    "\n",
    "    def shutdown(self):\n",
    "        logger.info(\"Shutting down tasks...\")\n",
    "        self.running.clear()\n",
    "        self.executor.shutdown(wait=True)\n",
    "        logger.info(\"Shutdown complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d62cc025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 19:25:05,044 - INFO - Using device=cuda dtype=torch.float16\n",
      "2025-09-10 19:25:05,044 - INFO - Using device=cuda dtype=torch.float16\n",
      "2025-09-10 19:25:05,047 - INFO - Loading Whisper model and processor...\n",
      "2025-09-10 19:25:05,047 - INFO - Loading Whisper model and processor...\n",
      "c:\\STT-TTS\\venv\\lib\\site-packages\\transformers\\processing_utils.py:1318: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "c:\\STT-TTS\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:4838: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "2025-09-10 19:25:12,003 - INFO - Loading TTS model...\n",
      "2025-09-10 19:25:12,003 - INFO - Loading TTS model...\n",
      "c:\\STT-TTS\\venv\\lib\\site-packages\\TTS\\api.py:70: UserWarning: `gpu` will be deprecated. Please use `tts.to(device)` instead.\n",
      "  warnings.warn(\"`gpu` will be deprecated. Please use `tts.to(device)` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/your_tts is already downloaded.\n",
      " > Using model: vits\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:0\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:None\n",
      " | > fft_size:1024\n",
      " | > power:None\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:None\n",
      " | > signal_norm:None\n",
      " | > symmetric_norm:None\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:None\n",
      " | > pitch_fmax:None\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\STT-TTS\\venv\\lib\\site-packages\\TTS\\utils\\io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Model fully restored. \n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:64\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:512\n",
      " | > power:1.5\n",
      " | > preemphasis:0.97\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:False\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:True\n",
      " | > db_level:-27.0\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:160\n",
      " | > win_length:400\n",
      " > External Speaker Encoder Loaded !!\n",
      " > initialization of language-embedding layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 19:25:13,674 - INFO - Default speaker: female-en-5\n",
      "2025-09-10 19:25:13,674 - INFO - Default speaker: female-en-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Model fully restored. \n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:64\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:512\n",
      " | > power:1.5\n",
      " | > preemphasis:0.97\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:False\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:True\n",
      " | > db_level:-27.0\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:160\n",
      " | > win_length:400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "2025-09-10 19:25:14,474 - WARNING - Warmup failed: Input length of decoder_input_ids is 4, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "2025-09-10 19:25:14,474 - WARNING - Warmup failed: Input length of decoder_input_ids is 4, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "2025-09-10 19:25:14,480 - INFO - Starting real-time loop. Press Stop to interrupt.\n",
      "2025-09-10 19:25:14,480 - INFO - Starting real-time loop. Press Stop to interrupt.\n",
      "2025-09-10 19:25:19,272 - INFO - Result [2025-09-10T13:55:19.272338]: Hi, hello. How are you?\n",
      "2025-09-10 19:25:19,272 - INFO - Result [2025-09-10T13:55:19.272338]: Hi, hello. How are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Hi, hello.', 'How are you?']\n",
      " > Processing time: 0.9971110820770264\n",
      " > Real-time factor: 0.35183877278653014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 19:25:20,597 - INFO - Result [2025-09-10T13:55:20.597146]: [BLANK_AUDIO]\n",
      "2025-09-10 19:25:20,597 - INFO - Result [2025-09-10T13:55:20.597146]: [BLANK_AUDIO]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['[BLANK_AUDIO]']\n",
      " > Processing time: 0.5074193477630615\n",
      " > Real-time factor: 0.3138029361552638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 19:25:22,088 - INFO - Result [2025-09-10T13:55:22.088496]: [BLANK_AUDIO]\n",
      "2025-09-10 19:25:22,088 - INFO - Result [2025-09-10T13:55:22.088496]: [BLANK_AUDIO]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['[BLANK_AUDIO]']\n",
      " > Processing time: 0.14701080322265625\n",
      " > Real-time factor: 0.08745437431448914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 19:25:26,645 - INFO - Result [2025-09-10T13:55:26.645775]: NARIO!\n",
      "2025-09-10 19:25:26,645 - INFO - Result [2025-09-10T13:55:26.645775]: NARIO!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['NARIO!']\n",
      " > Processing time: 0.12394928932189941\n",
      " > Real-time factor: 0.09440159125811075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 19:25:31,058 - INFO - Result [2025-09-10T13:55:31.058498]: Tomorrow, I...\n",
      "2025-09-10 19:25:31,058 - INFO - Result [2025-09-10T13:55:31.058498]: Tomorrow, I...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Tomorrow, I...']\n",
      " > Processing time: 0.11621379852294922\n",
      " > Real-time factor: 0.08640431116947897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 19:25:32,615 - INFO - Result [2025-09-10T13:55:32.615996]: It's very difficult.\n",
      "2025-09-10 19:25:32,615 - INFO - Result [2025-09-10T13:55:32.615996]: It's very difficult.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "[\"It's very difficult.\"]\n",
      " > Processing time: 0.12345027923583984\n",
      " > Real-time factor: 0.05588514225253048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 19:25:39,725 - INFO - Result [2025-09-10T13:55:39.725347]: (speaking in foreign language)\n",
      "2025-09-10 19:25:39,725 - INFO - Result [2025-09-10T13:55:39.725347]: (speaking in foreign language)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['(speaking in foreign language)']\n",
      " > Processing time: 0.12999248504638672\n",
      " > Real-time factor: 0.051728008375004665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 19:25:41,698 - INFO - Shutting down tasks...\n",
      "2025-09-10 19:25:41,698 - INFO - Shutting down tasks...\n",
      "2025-09-10 19:25:42,719 - INFO - Shutdown complete.\n",
      "2025-09-10 19:25:42,719 - INFO - Shutdown complete.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m parse_args()\n\u001b[0;32m      2\u001b[0m rt \u001b[38;5;241m=\u001b[39m RealtimeTranslator(args)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 139\u001b[0m, in \u001b[0;36mRealtimeTranslator.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m input_features \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39minput_features\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 139\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_kw)\n\u001b[0;32m    141\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text:\n",
      "File \u001b[1;32mc:\\STT-TTS\\venv\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:866\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[1;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, monitor_progress, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m             proc\u001b[38;5;241m.\u001b[39mset_begin_index(decoder_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    859\u001b[0m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[0;32m    860\u001b[0m (\n\u001b[0;32m    861\u001b[0m     seek_sequences,\n\u001b[0;32m    862\u001b[0m     seek_outputs,\n\u001b[0;32m    863\u001b[0m     should_skip,\n\u001b[0;32m    864\u001b[0m     do_condition_on_prev_tokens,\n\u001b[0;32m    865\u001b[0m     model_output_type,\n\u001b[1;32m--> 866\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "File \u001b[1;32mc:\\STT-TTS\\venv\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:1038\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate_with_fallback\u001b[1;34m(self, segment_input, decoder_input_ids, cur_bsz, seek, batch_idx_map, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1034\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m   1035\u001b[0m             generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, batch_size \u001b[38;5;241m-\u001b[39m cur_bsz), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1036\u001b[0m         )\n\u001b[1;32m-> 1038\u001b[0m seek_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1039\u001b[0m     segment_input,\n\u001b[0;32m   1040\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   1041\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1042\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1043\u001b[0m     prefix_allowed_tokens_fn\u001b[38;5;241m=\u001b[39mprefix_allowed_tokens_fn,\n\u001b[0;32m   1044\u001b[0m     synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1045\u001b[0m     decoder_input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m   1046\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs,\n\u001b[0;32m   1048\u001b[0m )\n\u001b[0;32m   1050\u001b[0m model_output_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "File \u001b[1;32mc:\\STT-TTS\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\STT-TTS\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:2551\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2539\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2540\u001b[0m         input_ids,\n\u001b[0;32m   2541\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2546\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2547\u001b[0m     )\n\u001b[0;32m   2549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2550\u001b[0m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2551\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2552\u001b[0m         input_ids,\n\u001b[0;32m   2553\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2554\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2555\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2556\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2557\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2558\u001b[0m     )\n\u001b[0;32m   2560\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2561\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m   2562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2563\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2564\u001b[0m     )\n",
      "File \u001b[1;32mc:\\STT-TTS\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:3275\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3270\u001b[0m n_eos_tokens \u001b[38;5;241m=\u001b[39m eos_token_id\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3271\u001b[0m beams_to_keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m n_eos_tokens) \u001b[38;5;241m*\u001b[39m num_beams\n\u001b[0;32m   3272\u001b[0m top_num_beam_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3273\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeams_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 3275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3277\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(cur_len, input_ids\u001b[38;5;241m.\u001b[39mdevice, model_kwargs)\n\u001b[0;32m   3279\u001b[0m \u001b[38;5;66;03m# (joao) feature lost in the refactor. Probably won't implement, hurts readability with minimal gains (there\u001b[39;00m\n\u001b[0;32m   3280\u001b[0m \u001b[38;5;66;03m# are newer low-memory alternatives like the offloaded cache)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "rt = RealtimeTranslator(args)\n",
    "rt.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
